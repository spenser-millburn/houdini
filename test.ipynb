{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\"\"\"\n",
    "TextGeneration Class Summary:\n",
    "\n",
    "The TextGeneration class facilitates text generation using the OpenAI GPT-3.5 model. \n",
    "\n",
    "Attributes:\n",
    "- client: An instance of the OpenAI API used for text generation.\n",
    "\n",
    "Methods:\n",
    "- __init__(self): Initializes the TextGeneration class with a specified topic for text generation.\n",
    "- generate_json(self, user_prompt, system_prompt=None): Generates a JSON response based on user prompts and an optional system prompt\n",
    "- generate_text(self, user_prompt, system_prompt=None): Generates a text-based response based on user prompts and an optional system prompt\n",
    "\n",
    "Usage:\n",
    "1. Instantiate TextGeneration \n",
    "2. Utilize the generate_json method to generate JSON responses based on user and system prompts.\n",
    "3. Utilize the generate_text method to generate text-based responses based on user and system prompts.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class GPTWrapper:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def generate_html(self, user_prompt, system_prompt=None, topic=\"everything\"):\n",
    "        if system_prompt is None:\n",
    "            system_prompt = f\"You are a helpful assistant that knows a lot about {topic} and only responds with HTML with only the HTML code. Nicely format with different header sizes and tables when necessary\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def generate_json(self, user_prompt, system_prompt=None, topic=\"everything\"):\n",
    "        if system_prompt is None:\n",
    "            system_prompt = f\"You are a helpful assistant that knows a lot about {topic} and only responds with JSON\"\n",
    "\n",
    "        return self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def generate_text(self, user_prompt, system_prompt=None, topic=\"everything\"):\n",
    "        if system_prompt is None:\n",
    "            system_prompt = f\"You are a helpful assistant that knows a lot about {topic}\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def generate_dockerfile(self, user_prompt, system_prompt=None, topic=\"everything\"):\n",
    "        if system_prompt is None:\n",
    "            system_prompt = f\"You are a helpful assistant that knows how to make a Dockerfile that will achieve the following output: {topic}. I would like you to respond with only the text for a Dockerfile nothing else. No extra code or files allowed, but you can write inline code in the Dockerfile. Ensure that the dockerfile is runnable\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip(\"`\").strip(\"Dockerfile\").strip('dockerfile')\n",
    "\n",
    "    def refine_dockerfile(self, user_prompt, dockerfile_string, build_logs, run_logs ,errors , topic=\"everything\"):\n",
    "\n",
    "        system_prompt = f\"You are a helpful assistant that knows how to improve this existing dockerfile ({dockerfile_string}) so that it will achieve the following output: {topic}. No extra code or files allowed, but you can write inline code in the Dockerfile. I would like you to respond with only the text for the improved Dockerfile nothing else. Ensure that the dockerfile is runnable. Here are the build logs from when I built the image: {build_logs} and the runtime logs: {run_logs}. If any, here are the errors that occurred: {errors}. Please take action on any feedback and fix any problems \"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip(\"`\").strip(\"Dockerfile\").strip('dockerfile')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProcessRunner Class\n",
    "Wrapper around docker to handle the contained process factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "import os\n",
    "from docker import DockerClient, errors\n",
    "\n",
    "class DockerRunner:\n",
    "    def __init__(self):\n",
    "        self.docker_client = DockerClient(base_url='unix://var/run/docker.sock')\n",
    "\n",
    "    def run_from_image_id(self, image_id, run_duration=5):\n",
    "        container = self.docker_client.containers.run(image_id, detach=True)\n",
    "        container.stop()\n",
    "        # Capture runtime logs\n",
    "        logs = container.logs(timestamps=True).decode('utf-8')\n",
    "        # Wait for the specified run duration\n",
    "        container.wait(timeout=run_duration)\n",
    "        container.remove()\n",
    "        response = {}\n",
    "        response[\"image\"] = image\n",
    "        response[\"run_logs\"] = logs\n",
    "        return response\n",
    "\n",
    "    def run_dockerfile(self, dockerfile_path, run_duration=5):\n",
    "        # Generate a random directory to store the Dockerfile\n",
    "        dir_path = f\"temp/{str(uuid.uuid4())}\"\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        temp_dockerfile_path = os.path.join(dir_path, \"Dockerfile\")\n",
    "        \n",
    "        # Prepare the response dictionary\n",
    "        response = {\"dockerfile\": dockerfile_path}\n",
    "        with open(dockerfile_path, 'r') as file: data = file.read() \n",
    "        response[\"dockerfile_content\"] = data\n",
    "        \n",
    "        try:\n",
    "            # Copy Dockerfile to the temporary directory\n",
    "            shutil.copy(dockerfile_path, temp_dockerfile_path)\n",
    "            \n",
    "            # Build the Docker image\n",
    "            # image, build_logs = self.docker_client.images.build(path=dir_path, rm=True, forcerm=True)\n",
    "            image, build_logs = self.docker_client.images.build(path=dir_path)\n",
    "            \n",
    "            # Run the container\n",
    "            container = self.docker_client.containers.run(image.id, detach=True)\n",
    "            \n",
    "            # Wait for the specified run duration\n",
    "            container.wait(timeout=run_duration)\n",
    "            \n",
    "            # Capture runtime logs\n",
    "            logs = container.logs(timestamps=True).decode('utf-8')\n",
    "\n",
    "            response[\"build_logs\"] = [log.get('stream', '') for log in build_logs if 'stream' in log]\n",
    "            response[\"image\"] = image\n",
    "            response[\"run_logs\"] = logs\n",
    "            \n",
    "            # Stop and remove the container\n",
    "            container.stop()\n",
    "            container.remove()\n",
    "            \n",
    "        except (errors.DockerException, errors.APIError) as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            response[\"error\"] = str(e)\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup: remove the directory after building\n",
    "            shutil.rmtree(dir_path)\n",
    "        \n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dockerfile_path = './Dockerfile'\n",
    "docker_runner = DockerRunner()\n",
    "result = docker_runner.run_dockerfile(dockerfile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <Image: ''>,\n",
       " 'run_logs': '2024-04-04T01:49:30.755416790Z Thu Apr  4 03:49:30 CEST 2024\\n'}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docker_runner.run_from_image_id(result[\"image\"].id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProcessBuilder Class\n",
    "\n",
    "Builds the process including documentation and any prerequisite information for the artifact (dockerized process) to be consumed by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pprint import pp\n",
    "\n",
    "class ProcessBuilder:\n",
    "    def __init__(self, llm, task : str, epochs=1 ):\n",
    "        self.llm = llm\n",
    "        self.task = task\n",
    "        self.epochs = epochs\n",
    "        self.dockerfile_path = './Dockerfile'\n",
    "        self.build_initial()\n",
    "\n",
    "    def build_initial(self):\n",
    "        dockerfile_lines = [self.llm.generate_dockerfile(self.task)]\n",
    "        with open(self.dockerfile_path, 'w') as file:\n",
    "            file.writelines(dockerfile_lines)\n",
    "\n",
    "    def run_dockerfile(self):\n",
    "        docker_runner = DockerRunner()\n",
    "        result = docker_runner.run_dockerfile(self.dockerfile_path)\n",
    "        return result\n",
    "\n",
    "    def iterate(self, new_result, user_feedback):\n",
    "        refined_dockerfile_lines = self.llm.refine_dockerfile(\n",
    "            user_feedback, \n",
    "            dockerfile_string=new_result.get('dockerfile_content'),\n",
    "            build_logs=new_result.get('build_logs not available', None),\n",
    "            run_logs=new_result.get('run_logs', \"run logs not available\"),\n",
    "            errors=new_result.get(\"error\", \"no errors\"),\n",
    "            topic=self.task\n",
    "        )\n",
    "        with open(self.dockerfile_path, 'w') as file:\n",
    "            file.write(refined_dockerfile_lines)\n",
    "        return self.run_dockerfile()\n",
    "\n",
    "    def documented(self, result):\n",
    "        result['documentation'] = self.llm.generate_text(f\"I am writing an api endpoint for the given task {self.task}, write a 10 word description for the enpoint including the request type, description and any inputs. Respond with only the 10 words\")\n",
    "        result['endpoint_name'] = self.llm.generate_text(f\"I am writing an api endpoint for the given task {self.task}, respond with the name of the endpoint only in one word \")\n",
    "        result['task'] = self.task\n",
    "        result['epochs'] = self.epochs\n",
    "        result.pop(\"build_logs\")\n",
    "        result.pop(\"run_logs\")\n",
    "        # result['run_logs'] = io.StringIO(result[\"run_logs\"])\n",
    "        # result['build_logs'] = io.StringIO(result[\"build_logs\"])\n",
    "        return result\n",
    "\n",
    "    def build_process(self) -> dict: \n",
    "        result = {'dockerfile_content': None}  # Initialize result dictionary\n",
    "        for _ in range(self.epochs):\n",
    "            feedback = self.llm.generate_text(f\"in 20 words describe what went wrong\")\n",
    "            result = self.iterate(new_result=result, user_feedback=feedback)\n",
    "            if 'error' not in result or not result['error']:  # Assuming 'error' key presence indicates issues\n",
    "                break  # Exit loop if no errors in the latest iteration\n",
    "        return self.documented(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dockerfile': './Dockerfile',\n",
      " 'dockerfile_content': 'FROM alpine:3.14\\n'\n",
      "                       '\\n'\n",
      "                       '# Set the timezone to Italy\\n'\n",
      "                       'RUN apk add --no-cache tzdata\\n'\n",
      "                       'ENV TZ=Europe/Rome\\n'\n",
      "                       '\\n'\n",
      "                       'CMD [\"date\"]',\n",
      " 'image': <Image: ''>,\n",
      " 'documentation': 'API endpoint to get Italy time, using GET request method.',\n",
      " 'endpoint_name': '\"italyTime\"',\n",
      " 'task': 'get the time in italy',\n",
      " 'epochs': 1}\n"
     ]
    }
   ],
   "source": [
    "process = ProcessBuilder(llm = GPTWrapper(), task = \"get the time in italy\",  epochs= 1).build_process()\n",
    "pp(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batch Process builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = [\n",
    "    \"generate a random number from 0 to 100\",\n",
    "    \"get time in italy\",\n",
    "    \"generate the first 16 numbers of pi\",\n",
    "    \"print the hex for the color blue\",\n",
    "    \"generate the first 16 numbers of pi\",\n",
    "]\n",
    "processes = [\n",
    "    ProcessBuilder(llm=GPTWrapper(), task=fn, epochs=3).build_process() for fn in fns\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AsyncFunctionGenerator\n",
    "generate a list of async functions that run the built images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Coroutine\n",
    "import asyncio\n",
    "\n",
    "class AsyncFunctionGenerator:\n",
    "    def __init__(self, function_names: List[str], image_list: List[str]):\n",
    "        self.functions: List[Callable[..., Coroutine]] = []\n",
    "        self._generate_functions(function_names, image_list=image_list)\n",
    "\n",
    "    def _generate_function(self, name: str, image: str) -> Callable[..., Coroutine]:\n",
    "        # Prepare the function body with the image variable correctly interpolated\n",
    "        async_template = f\"\"\"\n",
    "async def {name}():\n",
    "    print(\"hello from '{image}'\")\n",
    "    import subprocess; subprocess.run([\"docker\", \"run\", \"{image}\"])\n",
    "\"\"\"\n",
    "        local_namespace = {}\n",
    "        # Pass the globals and local_namespace as the environment for the exec\n",
    "        exec(async_template, globals(), local_namespace)\n",
    "        # The created function is now stored in the local_namespace with the key as its name\n",
    "        return local_namespace[name]\n",
    "\n",
    "    def _generate_functions(self, function_names: List[str], image_list: List[str]):\n",
    "        for name, image in zip(function_names, image_list):\n",
    "            # Generate a function for each name, passing along the corresponding image string\n",
    "            self.functions.append(self._generate_function(name, image))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## DOCKERFILES ############\n",
      "------------------\n",
      "'FROM alpine:latest\\n\\nCMD [\"sh\", \"-c\", \"echo $(( $RANDOM % 101 ))\"]'\n",
      "------------------\n",
      "------------------\n",
      "('FROM alpine\\n'\n",
      " '\\n'\n",
      " 'RUN apk add --no-cache tzdata\\n'\n",
      " 'ENV TZ=Europe/Rome\\n'\n",
      " '\\n'\n",
      " 'CMD [\"date\"]')\n",
      "------------------\n",
      "------------------\n",
      "('FROM alpine:latest\\n'\n",
      " '\\n'\n",
      " 'RUN apk add --no-cache bc\\n'\n",
      " '\\n'\n",
      " 'CMD echo \"scale=16; a(1)*4\" | bc -')\n",
      "------------------\n",
      "------------------\n",
      "\"FROM alpine\\nRUN echo -n '#0000FF'\"\n",
      "------------------\n",
      "------------------\n",
      "'FROM alpine\\n\\nRUN apk add --no-cache bc\\n\\nCMD echo \"scale=16; 4*a(1)\" | bc -'\n",
      "------------------\n",
      "########## FUNCTION:imageid ############\n",
      "{'randomNumberEndpoint': 'sha256:813c9adc9b04112e86678071ab23941ddb3f4cafcf81e8e10adf6e344a3763d4',\n",
      " 'timeInItaly': 'sha256:e96b2129db1844ccdb2a573e45c2023de633f490ba9f147d1f30ccbb7ede2d72',\n",
      " 'piEndpoint': 'sha256:0609121db6f4ba7fae86bc3fb482fbef7cb126e6800a3f8d26ee022f123591c7',\n",
      " 'colorHex': 'sha256:2987ab2a8c4ac0d8f180632c4fe4c82f0d41458f8ae332297329a52e661df1eb'}\n"
     ]
    }
   ],
   "source": [
    "# [process for process in processes]\n",
    "files= [process[\"dockerfile_content\"].strip(\"\\\"\") for process in processes]\n",
    "print(\"########## DOCKERFILES ############\")\n",
    "for file in files:\n",
    "  print(\"------------------\")\n",
    "  pp(file)\n",
    "  print(\"------------------\")\n",
    "\n",
    "print(\"########## FUNCTION:imageid ############\")\n",
    "pp({ process[\"endpoint_name\"].strip(\"\\\"\") : process[\"image\"] .id for process in processes} )\n",
    "\n",
    "images = [process[\"image\"].id for process in processes]\n",
    "names = [process[\"endpoint_name\"].strip(\"\\\"\") for process in processes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/randomNumberEndpoint': <function __main__.randomNumberEndpoint()>,\n",
       " '/timeInItaly': <function __main__.timeInItaly()>,\n",
       " '/piEndpoint': <function __main__.piEndpoint()>,\n",
       " '/colorHex': <function __main__.colorHex()>}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "generator = AsyncFunctionGenerator(names, images)\n",
    "\n",
    "async def main():\n",
    "    for func in generator.functions:\n",
    "        await func()  # This will execute each function and print the message\n",
    "\n",
    "# Corrected the execution with the fixed class definition\n",
    "await main()\n",
    "# generator.functions[1].__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoudiniApi\n",
    "The final product: an API built purely from prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from fastapi import FastAPI, APIRouter\n",
    "\n",
    "# Define your endpoint functions outside the class\n",
    "async def root():\n",
    "    return {\"message\": \"Hello World from the function\"}\n",
    "\n",
    "async def run_subprocess():\n",
    "    # Example: Running 'echo' command. Replace with your actual command\n",
    "    process = await asyncio.create_subprocess_shell(\n",
    "        'echo \"Hello from subprocess\"',\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "    stdout, stderr = await process.communicate()\n",
    "\n",
    "    if stderr:\n",
    "        return {\"error\": stderr.decode()}\n",
    "    \n",
    "    return {\"message\": stdout.decode().strip()}\n",
    "\n",
    "class APIEndpoints:\n",
    "    def __init__(self, endpoint_functions):\n",
    "        self.router = APIRouter()\n",
    "        self.endpoint_functions = endpoint_functions\n",
    "        self._register_routes()\n",
    "\n",
    "    def _register_routes(self):\n",
    "        # Iterate through the list of functions passed to the constructor\n",
    "        for path, func in self.endpoint_functions.items():\n",
    "            self.router.add_api_route(path, self._wrap_async(func), methods=[\"GET\"])\n",
    "\n",
    "    def _wrap_async(self, func):\n",
    "        # This wrapper converts the standalone async function to a method that can be called on the class instance.\n",
    "        async def wrapped():\n",
    "            return await func()\n",
    "        return wrapped\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/randomNumberEndpoint': <function __main__.randomNumberEndpoint()>,\n",
       " '/timeInItaly': <function __main__.timeInItaly()>,\n",
       " '/piEndpoint': <function __main__.piEndpoint()>,\n",
       " '/colorHex': <function __main__.colorHex()>}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create an instance of your APIEndpoints class, passing a dictionary of endpoint functions\n",
    "api_endpoints = APIEndpoints({ \"/\"+fn.__name__:fn  for fn in generator.functions})\n",
    "# Include the routes from the APIEndpoints instance into your FastAPI app\n",
    "api_endpoints.endpoint_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app.include_router(api_endpoints.router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
